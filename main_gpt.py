import json
import pandas as pd
import os
import openai
import glob
import tiktoken
import time
import zipfile
import requests
import xml.etree.ElementTree as ET
import re
from langchain.text_splitter import RecursiveCharacterTextSplitter

# â–¶ï¸ DART API í‚¤ ì„¤ì •
DART_API_KEY = "DART API KEY"
CORP_CODE_URL = f"https://opendart.fss.or.kr/api/corpCode.xml?crtfc_key={DART_API_KEY}"
ZIP_FILE = "corp_code.zip"
XML_FILE = "CORPCODE.xml"

# â–¶ï¸ OpenAI API í‚¤ ì„¤ì •
openai.api_key = "OPENAI API KEY"
openai.organization = "ORGANIZATION"

# â–¶ï¸ JSON ë° ê²°ê³¼ í´ë” ê²½ë¡œ
json_folder = r"json í´ë” ê²½ë¡œ ì…ë ¥"
output_folder = r"ê²°ê³¼ í´ë” ê²½ë¡œ ì…ë ¥"
os.makedirs(output_folder, exist_ok=True)

# â–¶ï¸ GPT í† í° ì œí•œ
TOKEN_LIMIT = 6000
tokenizer = tiktoken.get_encoding("cl100k_base")

# â–¶ï¸ DART ì¢…ëª©ì½”ë“œ â†’ ê¸°ì—…ëª… ë§¤í•‘
def download_and_extract_corpcode():
    if not os.path.exists(XML_FILE):
        print("\ud83d\udce6 DART \uae30\uc5b5 \ucf54\ub4dc zip \ub2e4\uc6b4\ub85c\ub4dc \uc911...")
        response = requests.get(CORP_CODE_URL)
        with open(ZIP_FILE, "wb") as f:
            f.write(response.content)
        with zipfile.ZipFile(ZIP_FILE, "r") as zip_ref:
            zip_ref.extractall()
        print("\u2705 CORPCODE.xml \ucd94\ucd9c \uc644\ub8cc")

def load_stock_code_to_company_name():
    stock_to_name = {}
    download_and_extract_corpcode()
    tree = ET.parse(XML_FILE)
    root = tree.getroot()

    for corp in root.findall("list"):
        stock_code = corp.find("stock_code").text
        corp_name = corp.find("corp_name").text
        if stock_code:
            stock_to_name[stock_code] = corp_name

    return stock_to_name

# â–¶ï¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
def preprocess_text(raw_text, max_tokens=TOKEN_LIMIT):
    token_count = len(tokenizer.encode(raw_text))
    if token_count <= max_tokens:
        return raw_text

    sentences = raw_text.split("\n")
    trimmed_text = []
    total_length = 0

    for sentence in sentences:
        sentence_length = len(tokenizer.encode(sentence))
        if total_length + sentence_length > max_tokens:
            break
        trimmed_text.append(sentence)
        total_length += sentence_length

    return "\n".join(trimmed_text)

# â–¶ï¸ GPT ì‘ë‹µ ë§ˆí¬ë‹¤ìš´ ì œê±° ë° JSON ë¸”ë¡ë§Œ ì¶”ì¶œ
def clean_response(response_text):
    text = re.sub(r"```json|```", "", response_text).strip()
    match = re.search(r"\{.*\}", text, re.DOTALL)
    if match:
        return match.group(0).strip()
    else:
        return text

# â–¶ï¸ GPT ë¶„ì„ í•¨ìˆ˜ (ìë™ ì¬ì‹œë„ í¬í•¨)
def analyze_text_with_gpt(company_name, raw_text, max_retry=3):
    trimmed_text = preprocess_text(raw_text)

    prompt = f"""
You are an AI assistant specializing in analyzing financial and business reports.
The text provided below is a **pre-downloaded offline business report**.
You must ONLY use the given text to extract information.
Do NOT try to browse the internet or assume you lack access to the data.
This is NOT real-time fetching â€” all necessary information is already included.

Your task is to extract **clear and specific company names** related to the target companyâ€™s supply chain and industry classification.
Accuracy is critical for investment and value chain analysis.

Extract in the exact JSON format:
{{
  "industry": "Main industry sector of the target company",
  "suppliers": [{{"category": "Raw material or service category", "company": "Exact name of supplier company"}}],
  "buyers": [{{"category": "Product or service type", "company": "Exact name of buyer company"}}]
}}

Rules:
- Do NOT include vague or group-like terms such as "various companies", "multiple clients", "domestic manufacturers", "ë¶ˆëª…", "í˜‘ë ¥ì—…ì²´", or anything similar.
- Only include **real, specific, and identifiable company names** like "Samsung Electronics", "SK hynix", etc.
- If the company name is not explicitly mentioned or is unclear, **exclude that entry entirely** from the output.
- Maintain the **original language** of the source content (Korean if written in Korean, English if written in English).
- Ensure that each entry for 'suppliers' and 'buyers' includes only **one company per category per line** to maintain clarity and precision in data extraction.
- **If multiple company names are separated by commas or "ë“±", extract each of them as a separate entry**, even if they appear in the same sentence or cell.

Example Output:
{{
  "industry": "ì² ê°•",
  "suppliers": [
    {{"category": "ì›ì¬ë£Œ", "company": "POSCO"}}
  ],
  "buyers": [
    {{"category": "ìë™ì°¨ ë¶€í’ˆ", "company": "í˜„ëŒ€ëª¨ë¹„ìŠ¤"}}
  ]
}}

{{
  "industry": "ë°˜ë„ì²´", 
  "suppliers": [ 
    {{"category": "ì›¨ì´í¼", "company": "SKì‹¤íŠ¸ë¡ "}}, 
    {{"category": "í¬í† ë ˆì§€ìŠ¤íŠ¸", "company": "ë™ì§„ì„ë¯¸ì¼"}} 
  ],
  "buyers": [ 
    {{"category": "ë°˜ë„ì²´", "company": "ì‚¼ì„±ì „ì"}}, 
    {{"category": "ë°˜ë„ì²´", "company": "TSMC"}} 
  ]
}}

Now analyze the following offline business report for: {company_name}

{trimmed_text}
"""

    attempt = 0
    while attempt < max_retry:
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You extract business relations and industry classification from financial reports."},
                    {"role": "user", "content": prompt}
                ]
            )
            raw_content = response.choices[0].message.content
            print("ğŸ“¤ GPT ì‘ë‹µ:")
            print(raw_content.encode("utf-8", errors="replace").decode("utf-8"))

            if any(kw in raw_content.lower() for kw in ["don't have access", "cannot access", "cannot browse"]):
                print("\u26a0\ufe0f \uc774\uc0c1 \uc751\ub2f5 \uac10\uc9c0, \uc7ac\uc2dc\ub3c4...")
                attempt += 1
                time.sleep(3)
                continue

            cleaned_content = clean_response(raw_content)
            parsed_json = json.loads(cleaned_content)
            return parsed_json, raw_content[:100]

        except json.JSONDecodeError:
            print("\u274c JSON \ud30c\uc2f1 \uc2e4\ud328. \uc7ac\uc2dc\ub3c4...")
            attempt += 1
            time.sleep(3)

        except openai.error.RateLimitError as e:
            wait_time = 30
            print(f"\u26a0\ufe0f Rate Limit \ubc1c\uc0dd, {wait_time}\ucd08 \ub300\uae30 \ud6c4 \uc7ac\uc2dc\ub3c4...")
            time.sleep(wait_time)

        except Exception as e:
            print(f"\u26a0\ufe0f GPT \uc694\uccad \uc911 \uc608\uc678 \ubc1c\uc0dd: {e}")
            attempt += 1
            time.sleep(3)

    print("\ud83d\udeab \ucd5c\ub300 \uc7ac\uc2dc\ub3c4 \ucd08\uacfc. \ubd84\uc11d \uc2e4\ud328.")
    return None, ""

# â–¶ï¸ ì „ì²´ JSON íŒŒì¼ ì²˜ë¦¬
if __name__ == "__main__":
    stock_to_name = load_stock_code_to_company_name()
    json_files = glob.glob(os.path.join(json_folder, "*.json"))
    total_files = len(json_files)

    fail_list = []

    for idx, json_file in enumerate(json_files, start=1):
        try:
            with open(json_file, "r", encoding="utf-8") as file:
                data = json.load(file)

            ticker_code = os.path.basename(json_file).replace(".json", "")
            company_name = stock_to_name.get(ticker_code, ticker_code)

            print(f"â–¶ [{idx}/{total_files}] Processing: {ticker_code} ({company_name})...")

            full_text = "\n".join([sec["text"] for sec in data.get("sections", [])])
            extracted_data, response_summary = analyze_text_with_gpt(company_name, full_text)

            if extracted_data is None:
                fail_list.append([ticker_code, company_name, "Parsing Failed or Empty Response", response_summary])
                continue

            industry_category = extracted_data.get("industry", "ê¸°íƒ€")
            processed_data = []

            for supplier in extracted_data.get("suppliers", []):
                processed_data.append([company_name, industry_category, "ê³µê¸‰ì²˜", supplier["category"], supplier["company"]])

            for buyer in extracted_data.get("buyers", []):
                processed_data.append([company_name, industry_category, "íŒë§¤ì²˜", buyer["category"], buyer["company"]])

            df = pd.DataFrame(processed_data, columns=["ì¢…ëª©ëª…", "ëŒ€ë¶„ë¥˜", "ì¤‘ë¶„ë¥˜", "ì†Œë¶„ë¥˜", "ì—°ê´€ê¸°ì—…"])
            excel_path = os.path.join(output_folder, f"{ticker_code}.xlsx")
            df.to_excel(excel_path, index=False)

            print(f"âœ… [{idx}/{total_files}] {ticker_code} ({company_name}): ì—‘ì…€ ì €ì¥ ì™„ë£Œ â†’ {excel_path}")

        except Exception as e:
            print(f"âš ï¸ [{idx}/{total_files}] {json_file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            ticker_code = os.path.basename(json_file).replace(".json", "")
            company_name = stock_to_name.get(ticker_code, ticker_code)
            fail_list.append([ticker_code, company_name, f"Exception: {str(e)}", ""])

    if fail_list:
        fail_df = pd.DataFrame(fail_list, columns=["ì¢…ëª©ì½”ë“œ", "ê¸°ì—…ëª…", "ì‹¤íŒ¨ ì´ìœ ", "GPT ì‘ë‹µ ìš”ì•½"])
        fail_path = os.path.join(output_folder, "fail_list.xlsx")
        fail_df.to_excel(fail_path, index=False)
        print(f"â— ì‹¤íŒ¨í•œ ì¢…ëª© {len(fail_list)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {fail_path}")
    else:
        print("âœ… ì‹¤íŒ¨í•œ ì¢…ëª© ì—†ì´ ì „ë¶€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")
